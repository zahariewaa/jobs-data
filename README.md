
# Job Scraping Dashboard

This is a Streamlit-based web application that scrapes job listings from the `dev.bg` website and displays interactive visualizations of the data. It automatically scrapes job data for various categories and provides filtering options to explore job listings.

## Features

- **Automatic Job Scraping on Startup**: The app scrapes job data from multiple categories (e.g., backend, frontend, fullstack) on startup or restart if no CSV files are found.
- **Manual Scraping**: Users can manually scrape new job data for a specific category via a button.
- **Interactive Data Visualization**:
    - View job listings in a table with various filters (job title, company name, tech stack, and date).
    - Bar charts showing the number of job offers by company.
    - Pie charts showing the tech stack distribution.

## Project Structure

- **`main.py`**: The main Streamlit app that handles scraping, data loading, filtering, and visualization.
- **`scrapper.py`**: A script that scrapes job listings from `dev.bg` for specific job categories and saves them to CSV files.

## Deployment

### 1. Deploy to Streamlit Cloud
1. Push your project to GitHub, including the following files:
   - `main.py`
   - `scrapper.py`
   - `requirements.txt` (see dependencies below)
2. Go to [Streamlit Cloud](https://streamlit.io/cloud) and create an account.
3. Click on **New app** and select your GitHub repository.
4. Choose the `main.py` file to run.
5. Streamlit Cloud will automatically install dependencies and deploy your app.

### 2. Dependencies
Make sure to include a `requirements.txt` file with the following dependencies:
```
streamlit
pandas
requests
beautifulsoup4
matplotlib
plotly
```

### 3. Running Locally
To run the project locally:
1. Clone the repository.
2. Install the dependencies using `pip install -r requirements.txt`.
3. Run the Streamlit app with the following command:
```bash
streamlit run main.py
```

## Usage

When the app runs, it will automatically scrape job data for the following categories on startup:
- Backend
- Frontend
- Fullstack
- PM/BA
- Quality Assurance
- Infrastructure
- Data Science
- UI/UX Arts
- Technical Support

The app provides a sidebar for filtering job data by title, company, date, and tech stack. Users can also manually trigger job scraping for a specific category.

## Limitations

- **No Persistent Storage**: CSV files generated by the app are not stored persistently when deployed on Streamlit Cloud. Each time the app restarts, the job data is scraped again.
- **Scraping Rate**: Frequent scraping could lead to performance issues or restrictions from the `dev.bg` website.

## Future Improvements

- Integrating cloud storage solutions for persistent data storage.
- Adding more job filters, such as location and seniority level.
- Enhancing visualizations with more detailed job metrics.

## License

This project is licensed under the MIT License.

